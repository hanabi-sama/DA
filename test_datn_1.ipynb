{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvZpXK5oJQaX",
        "outputId": "7190ba4d-c781-404e-81ff-0851368b68fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y6PBYaoSuep"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "from skimage import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqH7E7L1AzrP"
      },
      "outputs": [],
      "source": [
        "def get_images(data_dir,number_of_samples):\n",
        "  image_path = []\n",
        "  for category in categories:\n",
        "    path = os.path.join(data_dir,category)\n",
        "    i = 1\n",
        "    for img in os.listdir(path):\n",
        "      if i > number_of_samples:\n",
        "        break\n",
        "      else:\n",
        "        image_path.append(os.path.join(path,img))\n",
        "        i += 1\n",
        "  return image_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SGl3Lr__z4h"
      },
      "outputs": [],
      "source": [
        "def visualize_dataset(image_path,rows,cols):\n",
        "  fig = plt.figure(figsize=(20,20))\n",
        "  for i in range(1,rows * cols + 1):\n",
        "    fig.add_subplot(rows,cols,i)\n",
        "    img_array = io.imread(image_path[i-1])\n",
        "    fig.subplots_adjust(hspace=1)\n",
        "    plt.imshow(cv2.cvtColor(img_array,cv2.COLOR_BGR2RGB))\n",
        "    plt.xlabel(image_path[i-1].split('/')[-2])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BGrmiYn6ap7",
        "outputId": "ffc4ef5c-b38b-4b76-9141-d34df8b1fed8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ],
      "source": [
        "rootdir = os.getcwd()\n",
        "dataset_dir = os.path.join(rootdir,'/content/gdrive/MyDrive/DATN/dataset/train')\n",
        "\n",
        "model_path = os.path.join(rootdir,'/content/gdrive/MyDrive/DATN/models/facenet_keras.h5')\n",
        "facenet_model = load_model(model_path)\n",
        "\n",
        "categories = os.listdir(dataset_dir)\n",
        "def check_pretrained_file(embeddings_model):\n",
        "\tdata = pickle.loads(open(embeddings_model,\"rb\").read())\n",
        "\tnames = np.array(data[\"names\"])\n",
        "\tunique_names = np.unique(names).tolist()\n",
        "\treturn [data,unique_names]\n",
        "\n",
        "def get_remaining_names(unique_names):\n",
        "\tremaining_names = np.setdiff1d(categories,unique_names).tolist()\n",
        "\treturn remaining_names\n",
        "\n",
        "def get_all_face_pixels():\n",
        "\timage_ids = []\n",
        "\timage_paths = []\n",
        "\timage_arrays = []\n",
        "\tnames = []\n",
        "\tfor category in categories:\n",
        "\t\tpath = os.path.join(dataset_dir,category)\n",
        "\t\tfor img in os.listdir(path):\n",
        "\t\t\timg_array = cv2.imread(os.path.join(path,img))\n",
        "\t\t\timage_paths.append(os.path.join(path,img))\n",
        "\t\t\timage_ids.append(img)\n",
        "\t\t\timage_arrays.append(img_array)\n",
        "\t\t\tnames.append(category)\n",
        "\treturn [image_ids,image_paths,image_arrays,names]\n",
        "\n",
        "\n",
        "def get_remaining_face_pixels(remaining_names):\n",
        "\timage_ids = []\n",
        "\timage_paths = []\n",
        "\timage_arrays = []\n",
        "\tnames = []\n",
        "\tface_ids = []\n",
        "\tif len(remaining_names) != 0:\t\n",
        "\t\tfor category in remaining_names:\n",
        "\t\t\tpath = os.path.join(dataset_dir,category)\n",
        "\t\t\tfor img in os.listdir(path):\n",
        "\t\t\t\timg_array = cv2.imread(os.path.join(path,img))\n",
        "\t\t\t\timage_paths.append(os.path.join(path,img))\n",
        "\t\t\t\timage_ids.append(img)\n",
        "\t\t\t\timage_arrays.append(img_array)\n",
        "\t\t\t\tnames.append(category)\n",
        "\t\treturn [image_ids,image_paths,image_arrays,names]\n",
        "\telse:\n",
        "\t\treturn None\n",
        "\n",
        "\n",
        "def normalize_pixels(imagearrays):\n",
        "\tface_pixels = np.array(imagearrays)\n",
        "\t# scale pixel values\n",
        "\tface_pixels = face_pixels.astype('float32')\n",
        "\t# standardize pixel values across channels (global)\n",
        "\tmean, std = face_pixels.mean(), face_pixels.std()\n",
        "\tface_pixels = (face_pixels - mean) / std\n",
        "\treturn face_pixels\n",
        "\n",
        "\n",
        "\n",
        "embeddings_model_file = os.path.join(rootdir,\"/content/gdrive/MyDrive/DATN/models/embeddings.pickle\")\n",
        "if not os.path.exists(embeddings_model_file):\n",
        "\t[image_ids,image_paths,image_arrays,names] = get_all_face_pixels()\n",
        "\tface_pixels = normalize_pixels(imagearrays = image_arrays)   \n",
        "\tembeddings = []\n",
        "\tfor (i,face_pixel) in enumerate(face_pixels):\n",
        "\t\tsample = np.expand_dims(face_pixel,axis=0)\n",
        "\t\tembedding = facenet_model.predict(sample)\n",
        "\t\tnew_embedding = embedding.reshape(-1)\n",
        "\t\tembeddings.append(new_embedding)\n",
        "\t\tdata = {\"paths\":image_paths, \"names\":names,\"imageIDs\":image_ids,\"embeddings\":embeddings}\n",
        "\tf = open('/content/gdrive/MyDrive/DATN/models/embeddings.pickle' , \"wb\")\n",
        "\tf.write(pickle.dumps(data))\n",
        "\tf.close()\n",
        "\n",
        "else:\n",
        "\t[old_data,unique_names] = check_pretrained_file(embeddings_model_file)\n",
        "\tremaining_names = get_remaining_names(unique_names)\n",
        "\tdata = get_remaining_face_pixels(remaining_names)\n",
        "\tif data != None:\n",
        "\t\t[image_ids,image_paths,image_arrays,names] = data\n",
        "\t\tface_pixels = normalize_pixels(imagearrays = image_arrays)\n",
        "\t\tembeddings = []\n",
        "\t\tfor (i,face_pixel) in enumerate(face_pixels):\n",
        "\t\t\tsample = np.expand_dims(face_pixel,axis=0)\n",
        "\t\t\tembedding = facenet_model.predict(sample)\n",
        "\t\t\tnew_embedding = embedding.reshape(-1)\n",
        "\t\t\tembeddings.append(new_embedding)\n",
        "\t\tnew_data = {\"paths\":image_paths, \"names\":names,\"imageIDs\":image_ids,\"embeddings\":embeddings}\n",
        "\t\tcombined_data = {\"paths\":[],\"names\":[],\"face_ids\":[],\"imageIDs\":[],\"embeddings\":[]}\n",
        "\t\tcombined_data[\"paths\"] = old_data[\"paths\"] + new_data[\"paths\"]\n",
        "\t\tcombined_data[\"names\"] = old_data[\"names\"] + new_data[\"names\"]\n",
        "\t\tcombined_data[\"face_ids\"] = old_data[\"face_ids\"] + new_data[\"face_ids\"]\n",
        "\t\tcombined_data[\"imageIDs\"] = old_data[\"imageIDs\"] + new_data[\"imageIDs\"]\n",
        "\t\tcombined_data[\"embeddings\"] = old_data[\"embeddings\"] + new_data[\"embeddings\"]\n",
        "\n",
        "\t\tf = open('/content/gdrive/MyDrive/DATN/models/embeddings.pickle' , \"wb\")\n",
        "\t\tf.write(pickle.dumps(combined_data))\n",
        "\t\tf.close()\n",
        "\telse:\n",
        "\t\tprint(\"No new data found... Embeddings has already extracted for this user\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoN06Z0ICVLw"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.calibration import CalibratedClassifierCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Yf2UQwrCZr4",
        "outputId": "721bc96c-3a8e-40d4-a228-d5cdc1a1584b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training done successfully\n"
          ]
        }
      ],
      "source": [
        "rootdir = os.getcwd()\n",
        "\n",
        "embeddings_path = os.path.join(rootdir,'/content/gdrive/MyDrive/DATN/models/embeddings.pickle')\n",
        "\n",
        "def load_embeddings_and_labels():\n",
        "    data = pickle.loads(open(embeddings_path, \"rb\").read())\n",
        "    # encoding labels by names\n",
        "    label = LabelEncoder()\n",
        "    names = np.array(data[\"names\"])                       \n",
        "    labels = label.fit_transform(names)\n",
        "    # getting names\n",
        "    # getting embeddings\n",
        "    Embeddings = np.array(data[\"embeddings\"])\n",
        "    return [label,labels,Embeddings,names]\n",
        "\n",
        "def create_svm_model(labels,embeddings):\n",
        "    model_svc = LinearSVC()\n",
        "    recognizer = CalibratedClassifierCV(model_svc)   \n",
        "    recognizer.fit(embeddings,labels)\n",
        "    return recognizer\n",
        "\n",
        "\n",
        "[label,labels,Embeddings,names] = load_embeddings_and_labels()\n",
        "recognizer = create_svm_model(labels=labels,embeddings=Embeddings)\n",
        "f1 = open('/content/gdrive/MyDrive/DATN/models/recognizer.pickle', \"wb\")\n",
        "f1.write(pickle.dumps(recognizer))\n",
        "f1.close()\n",
        "print(\"Training done successfully\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "test_datn_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}